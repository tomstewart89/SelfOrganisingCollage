{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/tom/repos/SelfOrganisingWeddingInvite/SelfOrganisingCollage')\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "import PIL\n",
    "\n",
    "from photo_library import PhotoLibrary\n",
    "from SOM import SelfOrganisingMap\n",
    "from feature_extraction import FeatureExtractor\n",
    "from draw_samples import sample_from_unit_cube\n",
    "from patch_worker import Patchworker\n",
    "from utils import ravel_index, simplify_shape\n",
    "\n",
    "args = lambda: None\n",
    "args.directory = '/home/tom/Pictures/test_pics'\n",
    "args.height = 48\n",
    "args.width = 27\n",
    "args.feature = 'mean_color'\n",
    "args.epochs = 20\n",
    "args.reuse_penalty = 100.\n",
    "args.sample_size = 1000\n",
    "args.border = 20\n",
    "args.magnify = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "library = PhotoLibrary(args.directory)\n",
    "extractor = FeatureExtractor.factory(args.feature)\n",
    "\n",
    "try:\n",
    "    with open(os.path.join(args.directory, args.feature), \"rb\") as f:\n",
    "        feature_dict = pickle.load(f)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    feature_dict = extractor.process_library(library)\n",
    "    with open(os.path.join(args.directory, args.feature), \"wb\") as f:\n",
    "        pickle.dump(feature_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som = SelfOrganisingMap(shape=[args.height, args.width, extractor.feature_dim], sigma=5., eta=1.)\n",
    "\n",
    "# draw a subset of just the most interesting the entire photo library\n",
    "sample, keys = extractor.draw_interesting_samples(feature_dict, args.sample_size)\n",
    "\n",
    "for i in range(args.epochs):\n",
    "    for j, feature in enumerate(sample[torch.randperm(args.sample_size),:]):\n",
    "        som.update(feature.cuda())\n",
    "        plt.imsave('som_training/som_%i_%i' % (i, j), torch.transpose(som.grid,0,1))\n",
    "        \n",
    "plt.imshow(torch.transpose(som.grid,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patch = Patchworker([args.height, args.width])\n",
    "times_used = torch.zeros(len(sample)).unsqueeze(1).cuda()\n",
    "layout = []\n",
    "\n",
    "# I can omit the occupied parts of the grid to speed this up\n",
    "while not patch.full():\n",
    "    dist = (sample.cuda().unsqueeze(1) - som.grid.reshape(-1,3).unsqueeze(0)).norm(dim=2)\n",
    "    dist += (times_used * args.reuse_penalty) # penalise the samples that have already been placed on the grid\n",
    "    dist[:, patch.occupied.reshape(-1)] = dist.max() # omit any neurons that already have a photo covering them\n",
    "\n",
    "    # find the neuron and sample that match most closely\n",
    "    winning_sample, winning_neuron = ravel_index(dist.argmin(), (som.grid.shape[0] * som.grid.shape[1]))\n",
    "    target_coord = torch.Tensor(ravel_index(winning_neuron, som.grid.shape[1]))\n",
    "    \n",
    "    try:\n",
    "        target_shape = simplify_shape(library[keys[winning_sample]].size, target_area=8)\n",
    "\n",
    "        # add the photo to the patch work\n",
    "        coord, shape = patch.add_patch(target_coord, target_shape)\n",
    "\n",
    "        layout.append([keys[winning_sample], coord, shape, sample[winning_sample]])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    times_used[winning_sample] += 1\n",
    "\n",
    "for key, coord, shape, feature in layout:\n",
    "    plt.scatter(coord[1], -coord[0], c=feature.numpy()[None,:])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchwork = torch.zeros([args.height, args.width, 3])\n",
    "\n",
    "for key, coord, shape, feature in layout:\n",
    "    tl = coord - shape / 2\n",
    "    br = tl + shape\n",
    "\n",
    "    patchwork[tl[0]:br[0],tl[1]:br[1],:] = feature\n",
    "\n",
    "plt.imshow(torch.transpose(patchwork,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canvas = Image.fromarray((som.grid.cpu().numpy() * 255).astype(np.uint8))\n",
    "canvas = canvas.resize((canvas.size[0]*args.magnify, canvas.size[1]*args.magnify),PIL.Image.BILINEAR)\n",
    "canvas = canvas.transpose(PIL.Image.TRANSPOSE)\n",
    "\n",
    "for i, (key, centroid, shape, feature) in tqdm(enumerate(layout)):\n",
    "    canvas.save('canvas_layout/canvas_%i.png' % i)\n",
    "\n",
    "    img = library[key]\n",
    "\n",
    "    size = shape.numpy() * args.magnify\n",
    "    corner = (centroid - shape / 2).numpy() * args.magnify\n",
    "    \n",
    "    target_aspect = size[0] / size[1]\n",
    "    img_aspect = img.size[0] / img.size[1]\n",
    "    \n",
    "    if target_aspect < img_aspect:\n",
    "        crop_size = (img.size[1], img.size[1] * target_aspect)\n",
    "    else:\n",
    "        crop_size = (img.size[0] / target_aspect, img.size[0])\n",
    "\n",
    "    crop = transforms.Compose([\n",
    "        transforms.CenterCrop(crop_size),\n",
    "        transforms.Resize((size[1], size[0]))\n",
    "    ])\n",
    "\n",
    "    canvas.paste(crop(img), tuple(corner.astype(np.int)))\n",
    "    \n",
    "plt.imshow(canvas)\n",
    "\n",
    "canvas.save('canvas_layout/final.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = torch.LongTensor([[0, 1, 1], [2, 0, 2]])\n",
    "v = torch.FloatTensor([3, 4, 5])\n",
    "ten = torch.sparse.FloatTensor(i, v, torch.Size([2,3]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "projectname",
   "language": "python",
   "name": "projectname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
